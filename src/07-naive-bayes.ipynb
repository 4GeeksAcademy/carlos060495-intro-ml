{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0110f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.stats import uniform\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2771786c",
   "metadata": {},
   "source": [
    "# 1.1 Importamos el dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd06bf30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>package_name</th>\n",
       "      <th>review</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>com.facebook.katana</td>\n",
       "      <td>privacy at least put some option appear offli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>com.facebook.katana</td>\n",
       "      <td>messenger issues ever since the last update, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>com.facebook.katana</td>\n",
       "      <td>profile any time my wife or anybody has more ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>com.facebook.katana</td>\n",
       "      <td>the new features suck for those of us who don...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>com.facebook.katana</td>\n",
       "      <td>forced reload on uploading pic on replying co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          package_name                                             review  \\\n",
       "0  com.facebook.katana   privacy at least put some option appear offli...   \n",
       "1  com.facebook.katana   messenger issues ever since the last update, ...   \n",
       "2  com.facebook.katana   profile any time my wife or anybody has more ...   \n",
       "3  com.facebook.katana   the new features suck for those of us who don...   \n",
       "4  com.facebook.katana   forced reload on uploading pic on replying co...   \n",
       "\n",
       "   polarity  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw/playstore_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da290e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6318eeb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>privacy at least put some option appear offli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>messenger issues ever since the last update, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>profile any time my wife or anybody has more ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the new features suck for those of us who don...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forced reload on uploading pic on replying co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  polarity\n",
       "0   privacy at least put some option appear offli...         0\n",
       "1   messenger issues ever since the last update, ...         0\n",
       "2   profile any time my wife or anybody has more ...         0\n",
       "3   the new features suck for those of us who don...         0\n",
       "4   forced reload on uploading pic on replying co...         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminamos la columna package_name\n",
    "df = df.drop(columns=['package_name'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849f65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos espacios y convertimos a min√∫sculas\n",
    "df[\"review\"] = df[\"review\"].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eefafa",
   "metadata": {},
   "source": [
    "# 2.1 Division Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5f01f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = df[\"review\"]\n",
    "polarity = df[\"polarity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c9c7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos los datos en conjuntos de entrenamiento y testeo\n",
    "X_train, X_test, y_train, y_test = train_test_split(review, polarity, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906903bb",
   "metadata": {},
   "source": [
    "# 3.1 Vectorizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0682f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab8f925",
   "metadata": {},
   "source": [
    "# 4.1 Modelo Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24de6693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90       126\n",
      "           1       0.84      0.58      0.69        53\n",
      "\n",
      "    accuracy                           0.84       179\n",
      "   macro avg       0.84      0.77      0.79       179\n",
      "weighted avg       0.84      0.84      0.83       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bayes_multinomial= MultinomialNB().fit(X_train_vec, y_train)\n",
    "y_pred_multinomial = bayes_multinomial.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred_multinomial))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5257a302",
   "metadata": {},
   "source": [
    "# 4.2 Modelo Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3e76cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86       126\n",
      "           1       0.70      0.60      0.65        53\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.77      0.75      0.76       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bayes_gaussian = GaussianNB().fit(X_train_vec.toarray(), y_train)\n",
    "y_pred_gaussian = bayes_gaussian.predict(X_test_vec.toarray())\n",
    "print(classification_report(y_test, y_pred_gaussian))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a3f404",
   "metadata": {},
   "source": [
    "# 4.3 Modelo Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b51ea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90       126\n",
      "           1       0.81      0.64      0.72        53\n",
      "\n",
      "    accuracy                           0.85       179\n",
      "   macro avg       0.84      0.79      0.81       179\n",
      "weighted avg       0.85      0.85      0.84       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bayes_bernoulli = BernoulliNB().fit(X_train_vec, y_train)\n",
    "y_pred_bernoulli = bayes_bernoulli.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred_bernoulli))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e43260",
   "metadata": {},
   "source": [
    "# 4.4 Comparativa modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cab7d13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Modelo  Accuracy  Precision  Recall  F1-Score\n",
      "0  MultinomialNB    0.8436     0.8429  0.8436    0.8343\n",
      "1     GaussianNB    0.8045     0.7987  0.8045    0.8002\n",
      "2    BernoulliNB    0.8492     0.8460  0.8492    0.8436\n",
      "\n",
      "üèÜ Mejor modelo: BernoulliNB con Accuracy: 0.8492\n"
     ]
    }
   ],
   "source": [
    "# Calculamos las m√©tricas para cada modelo\n",
    "modelos = ['MultinomialNB', 'GaussianNB', 'BernoulliNB']\n",
    "predicciones = [y_pred_multinomial, y_pred_gaussian, y_pred_bernoulli]\n",
    "\n",
    "resultados = []\n",
    "for modelo, pred in zip(modelos, predicciones):\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred, average='weighted')\n",
    "    recall = recall_score(y_test, pred, average='weighted')\n",
    "    f1 = f1_score(y_test, pred, average='weighted')\n",
    "    \n",
    "    resultados.append({\n",
    "        'Modelo': modelo,\n",
    "        'Accuracy': round(accuracy, 4),\n",
    "        'Precision': round(precision, 4),\n",
    "        'Recall': round(recall, 4),\n",
    "        'F1-Score': round(f1, 4)\n",
    "    })\n",
    "\n",
    "# Creamos un DataFrame con los resultados\n",
    "comparacion = pd.DataFrame(resultados)\n",
    "print(comparacion)\n",
    "\n",
    "# Identificamos el mejor modelo por accuracy\n",
    "mejor_modelo = comparacion.loc[comparacion['Accuracy'].idxmax()]\n",
    "print(f\"\\nüèÜ Mejor modelo: {mejor_modelo['Modelo']} con Accuracy: {mejor_modelo['Accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98040578",
   "metadata": {},
   "source": [
    "# 5.1 Hiperparametros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4dc95",
   "metadata": {},
   "source": [
    "# 5.1.2 B√öSQUEDA EXTENSIVA CON GRIDSEARCHCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ce858c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando mejores hiperpar√°metros para BernoulliNB...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "üéØ Mejores hiperpar√°metros: {'alpha': 1.0, 'binarize': 0.0, 'fit_prior': False}\n",
      "üìä Mejor score (accuracy): 0.8132\n",
      "\n",
      "üìà Classification Report del modelo optimizado:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90       126\n",
      "           1       0.80      0.68      0.73        53\n",
      "\n",
      "    accuracy                           0.85       179\n",
      "   macro avg       0.84      0.80      0.82       179\n",
      "weighted avg       0.85      0.85      0.85       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definimos el espacio de hiperpar√°metros para BernoulliNB\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 1.5, 2.0],  # Suavizado de Laplace\n",
    "    'fit_prior': [True, False],  # Si debe aprender las probabilidades a priori\n",
    "    'binarize': [0.0, 0.5, 1.0]  # Umbral para binarizaci√≥n de features\n",
    "}\n",
    "\n",
    "# Creamos el GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=BernoulliNB(),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # Validaci√≥n cruzada con 5 folds\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Usa todos los procesadores disponibles\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Entrenamos con GridSearch\n",
    "print(\"Buscando mejores hiperpar√°metros para BernoulliNB...\")\n",
    "grid_search.fit(X_train_vec, y_train)\n",
    "\n",
    "# Mostramos los mejores par√°metros\n",
    "print(f\"\\nüéØ Mejores hiperpar√°metros: {grid_search.best_params_}\")\n",
    "print(f\"üìä Mejor score (accuracy): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluamos el modelo optimizado en el conjunto de prueba\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_optimized = best_model.predict(X_test_vec)\n",
    "\n",
    "print(\"\\nüìà Classification Report del modelo optimizado:\")\n",
    "print(classification_report(y_test, y_pred_optimized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257611f0",
   "metadata": {},
   "source": [
    "# 5.1.3 Busqueda intensiva Randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4b62c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando mejores hiperpar√°metros con RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "\n",
      "üéØ Mejores hiperpar√°metros: {'alpha': np.float64(0.787354579378964), 'binarize': np.float64(0.40702354766084387), 'fit_prior': True}\n",
      "üìä Mejor score (accuracy): 0.8258\n",
      "\n",
      "üìà Classification Report del modelo optimizado:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       126\n",
      "           1       0.79      0.70      0.74        53\n",
      "\n",
      "    accuracy                           0.85       179\n",
      "   macro avg       0.83      0.81      0.82       179\n",
      "weighted avg       0.85      0.85      0.85       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definimos distribuciones de hiperpar√°metros para BernoulliNB\n",
    "param_distributions = {\n",
    "    'alpha': uniform(0.01, 2.0),  # Distribuci√≥n uniforme entre 0.01 y 2.01 (loc + scale)\n",
    "    'fit_prior': [True, False],\n",
    "    'binarize': uniform(0.0, 1.5)  # Distribuci√≥n uniforme entre 0.0 y 1.5 (loc + scale)\n",
    "}\n",
    "\n",
    "# Creamos el RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=BernoulliNB(),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,  # N√∫mero de combinaciones aleatorias a probar\n",
    "    cv=5,  # Validaci√≥n cruzada con 5 folds\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenamos con RandomizedSearch\n",
    "print(\"Buscando mejores hiperpar√°metros con RandomizedSearchCV...\")\n",
    "random_search.fit(X_train_vec, y_train)\n",
    "\n",
    "# Mostramos los mejores par√°metros\n",
    "print(f\"\\nüéØ Mejores hiperpar√°metros: {random_search.best_params_}\")\n",
    "print(f\"üìä Mejor score (accuracy): {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluamos el modelo optimizado en el conjunto de prueba\n",
    "best_model_random = random_search.best_estimator_\n",
    "y_pred_random = best_model_random.predict(X_test_vec)\n",
    "\n",
    "print(\"\\nüìà Classification Report del modelo optimizado:\")\n",
    "print(classification_report(y_test, y_pred_random))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6b8c2",
   "metadata": {},
   "source": [
    "# 6.1 Comparacion modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fba9a048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Comparaci√≥n de modelos BernoulliNB:\n",
      "                           Modelo  Accuracy  Precision  Recall  F1-Score\n",
      "0                BernoulliNB Base    0.8492     0.8460  0.8492    0.8436\n",
      "1        BernoulliNB + GridSearch    0.8547     0.8515  0.8547    0.8511\n",
      "2  BernoulliNB + RandomizedSearch    0.8547     0.8517  0.8547    0.8521\n",
      "\n",
      "üèÜ Mejor configuraci√≥n: BernoulliNB + GridSearch\n",
      "   Accuracy: 0.8547\n",
      "   F1-Score: 0.8511\n",
      "\n",
      "üìà Mejora con GridSearch: +0.55%\n",
      "üìà Mejora con RandomizedSearch: +0.55%\n"
     ]
    }
   ],
   "source": [
    "# Comparamos el modelo base vs los optimizados\n",
    "modelos_comparacion = ['BernoulliNB Base', 'BernoulliNB + GridSearch', 'BernoulliNB + RandomizedSearch']\n",
    "predicciones_comparacion = [y_pred_bernoulli, y_pred_optimized, y_pred_random]\n",
    "\n",
    "resultados_comparacion = []\n",
    "for modelo, pred in zip(modelos_comparacion, predicciones_comparacion):\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred, average='weighted')\n",
    "    recall = recall_score(y_test, pred, average='weighted')\n",
    "    f1 = f1_score(y_test, pred, average='weighted')\n",
    "    \n",
    "    resultados_comparacion.append({\n",
    "        'Modelo': modelo,\n",
    "        'Accuracy': round(accuracy, 4),\n",
    "        'Precision': round(precision, 4),\n",
    "        'Recall': round(recall, 4),\n",
    "        'F1-Score': round(f1, 4)\n",
    "    })\n",
    "\n",
    "# Creamos un DataFrame con los resultados\n",
    "comparacion_final = pd.DataFrame(resultados_comparacion)\n",
    "print(\"üìä Comparaci√≥n de modelos BernoulliNB:\")\n",
    "print(comparacion_final)\n",
    "\n",
    "# Identificamos el mejor modelo\n",
    "mejor_modelo_final = comparacion_final.loc[comparacion_final['Accuracy'].idxmax()]\n",
    "print(f\"\\nüèÜ Mejor configuraci√≥n: {mejor_modelo_final['Modelo']}\")\n",
    "print(f\"   Accuracy: {mejor_modelo_final['Accuracy']}\")\n",
    "print(f\"   F1-Score: {mejor_modelo_final['F1-Score']}\")\n",
    "\n",
    "# Calculamos la mejora respecto al modelo base\n",
    "mejora_grid = (comparacion_final.iloc[1]['Accuracy'] - comparacion_final.iloc[0]['Accuracy']) * 100\n",
    "mejora_random = (comparacion_final.iloc[2]['Accuracy'] - comparacion_final.iloc[0]['Accuracy']) * 100\n",
    "\n",
    "print(f\"\\nüìà Mejora con GridSearch: {mejora_grid:+.2f}%\")\n",
    "print(f\"üìà Mejora con RandomizedSearch: {mejora_random:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f188e40",
   "metadata": {},
   "source": [
    "# 7.1 Guardamos el modelo con las mejores metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40e69b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo guardado: bernoulli_gridsearch.pkl\n",
      "‚úÖ Vectorizador guardado: vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "# Guardamos el mejor modelo (BernoulliNB + GridSearch) y el vectorizador\n",
    "joblib.dump(best_model, '../models/bernoulli_gridsearch.pkl')\n",
    "joblib.dump(vectorizer, '../models/vectorizer.pkl')\n",
    "\n",
    "print(\"‚úÖ Modelo guardado: bernoulli_gridsearch.pkl\")\n",
    "print(\"‚úÖ Vectorizador guardado: vectorizer.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
